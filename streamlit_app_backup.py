import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime, timedelta
import warnings
import os
import glob
from io import BytesIO
warnings.filterwarnings('ignore')

# Import custom modules
from data_processor import SafetyDataProcessor
from dashboard_components import DashboardComponents
from gemini_chatbot import create_chatbot_interface

# Page configuration
st.set_page_config(
    page_title="Ù„ÙˆØ­Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„",
    page_icon="ğŸ›¡ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .metric-card {
        background: linear-gradient(135deg, #f0f2f6 0%, #e8ecf0 100%);
        padding: 1.5rem;
        border-radius: 0.8rem;
        border-left: 4px solid #1f77b4;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        margin-bottom: 1rem;
    }
    .kpi-container {
        display: flex;
        justify-content: space-around;
        margin: 2rem 0;
        gap: 1rem;
    }
    .sidebar .sidebar-content {
        background-color: #f8f9fa;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 2px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 20px;
        padding-right: 20px;
        background-color: #f0f2f6;
        border-radius: 4px 4px 0px 0px;
    }
    .stTabs [aria-selected="true"] {
        background-color: #1f77b4;
        color: white;
    }
    .upload-section {
        background-color: #f8f9fa;
        padding: 2rem;
        border-radius: 0.8rem;
        border: 2px dashed #1f77b4;
        margin: 1rem 0;
    }
    .success-message {
        background-color: #d4edda;
        color: #155724;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #28a745;
        margin: 1rem 0;
    }
    .warning-message {
        background-color: #fff3cd;
        color: #856404;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #ffc107;
        margin: 1rem 0;
    }
    .info-box {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #2196f3;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def load_and_process_data():
    """Load and process all data using the SafetyDataProcessor"""
    processor = SafetyDataProcessor()
    
    # Load Excel data
    excel_data = {}
    if os.path.exists('sample-of-data.xlsx'):
        excel_data = processor.load_excel_data('sample-of-data.xlsx')
    
    # Load CSV data
    csv_data = {}
    csv_files = glob.glob('*.csv')
    for csv_file in csv_files:
        if csv_file != 'Power_BI_Copy_v.02_Sheet1.csv':  # Skip layout file
            data = processor.load_csv_data(csv_file)
            if not data.empty:
                csv_data[csv_file.replace('.csv', '')] = data
    
    # Combine all data sources
    all_data_sources = {**excel_data, **csv_data}
    
    # Create unified dataset
    unified_data = processor.create_unified_dataset(all_data_sources)
    
    # Generate KPIs
    kpi_data = processor.generate_kpi_data(unified_data)
    
    # Generate quality report
    quality_report = processor.get_data_quality_report(unified_data)
    
    return processor, unified_data, kpi_data, quality_report

def create_manual_upload_section():
    """Create manual data upload section"""
    st.header("ğŸ“¤ Ø±ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙŠØ¯ÙˆÙŠØ§Ù‹")
    
    with st.container():
        st.markdown('<div class="upload-section">', unsafe_allow_html=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Ø±ÙØ¹ Ù…Ù„Ù Excel")
            uploaded_excel = st.file_uploader(
                "Ø§Ø®ØªØ± Ù…Ù„Ù Excel",
                type=['xlsx', 'xls'],
                help="ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ù…Ù„Ù Excel ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ø¯Ø© Ø£ÙˆØ±Ø§Ù‚ Ø¹Ù…Ù„"
            )
            
            if uploaded_excel is not None:
                try:
                    # Save uploaded file temporarily
                    with open(f"temp_{uploaded_excel.name}", "wb") as f:
                        f.write(uploaded_excel.getbuffer())
                    
                    # Process the uploaded file
                    processor = SafetyDataProcessor()
                    new_data = processor.load_excel_data(f"temp_{uploaded_excel.name}")
                    
                    if new_data:
                        st.success(f"ØªÙ… Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø¬Ø§Ø­! ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(new_data)} ÙˆØ±Ù‚Ø© Ø¹Ù…Ù„")
                        
                        # Show preview
                        for sheet_name, df in new_data.items():
                            with st.expander(f"Ù…Ø¹Ø§ÙŠÙ†Ø©: {sheet_name}"):
                                st.dataframe(df.head(), use_container_width=True)
                        
                        # Update session state
                        if 'uploaded_data' not in st.session_state:
                            st.session_state.uploaded_data = {}
                        st.session_state.uploaded_data.update(new_data)
                        
                        if st.button("Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"):
                            st.session_state.data_updated = True
                            st.success("ØªÙ… Ø¯Ù…Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ù†Ø¬Ø§Ø­!")
                            st.rerun()
                    
                    # Clean up temporary file
                    os.remove(f"temp_{uploaded_excel.name}")
                    
                except Exception as e:
                    st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù: {str(e)}")
        
        with col2:
            st.subheader("Ø±ÙØ¹ Ù…Ù„ÙØ§Øª CSV")
            uploaded_csvs = st.file_uploader(
                "Ø§Ø®ØªØ± Ù…Ù„ÙØ§Øª CSV",
                type=['csv'],
                accept_multiple_files=True,
                help="ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª CSV ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª"
            )
            
            if uploaded_csvs:
                try:
                    processor = SafetyDataProcessor()
                    new_csv_data = {}
                    
                    for uploaded_csv in uploaded_csvs:
                        # Save temporarily and process
                        with open(f"temp_{uploaded_csv.name}", "wb") as f:
                            f.write(uploaded_csv.getbuffer())
                        
                        df = processor.load_csv_data(f"temp_{uploaded_csv.name}")
                        if not df.empty:
                            new_csv_data[uploaded_csv.name.replace('.csv', '')] = df
                        
                        # Clean up
                        os.remove(f"temp_{uploaded_csv.name}")
                    
                    if new_csv_data:
                        st.success(f"ØªÙ… Ø±ÙØ¹ {len(new_csv_data)} Ù…Ù„Ù CSV Ø¨Ù†Ø¬Ø§Ø­!")
                        
                        # Show preview
                        for file_name, df in new_csv_data.items():
                            with st.expander(f"Ù…Ø¹Ø§ÙŠÙ†Ø©: {file_name}"):
                                st.dataframe(df.head(), use_container_width=True)
                        
                        # Update session state
                        if 'uploaded_data' not in st.session_state:
                            st.session_state.uploaded_data = {}
                        st.session_state.uploaded_data.update(new_csv_data)
                        
                        if st.button("Ø¯Ù…Ø¬ Ù…Ù„ÙØ§Øª CSV"):
                            st.session_state.data_updated = True
                            st.success("ØªÙ… Ø¯Ù…Ø¬ Ù…Ù„ÙØ§Øª CSV Ø¨Ù†Ø¬Ø§Ø­!")
                            st.rerun()
                
                except Exception as e:
                    st.error(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª CSV: {str(e)}")
        
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Data validation section
        st.subheader("ğŸ” Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        if st.button("ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"):
            if 'uploaded_data' in st.session_state:
                processor = SafetyDataProcessor()
                quality_report = processor.get_data_quality_report(st.session_state.uploaded_data)
                
                st.subheader("ØªÙ‚Ø±ÙŠØ± Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
                
                for data_type, report in quality_report.items():
                    with st.expander(f"ØªÙ‚Ø±ÙŠØ±: {data_type}"):
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            st.metric("Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ", f"{report['total_rows']:,}")
                        with col2:
                            st.metric("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©", report['total_columns'])
                        with col3:
                            st.metric("Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©", f"{report['missing_data_percentage']:.1f}%")
                        
                        if report['duplicate_rows'] > 0:
                            st.warning(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {report['duplicate_rows']} ØµÙ Ù…ÙƒØ±Ø±")
                        else:
                            st.success("Ù„Ø§ ØªÙˆØ¬Ø¯ ØµÙÙˆÙ Ù…ÙƒØ±Ø±Ø©")
            else:
                st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø±ÙÙˆØ¹Ø© Ù„Ù„ÙØ­Øµ")

def create_main_dashboard(unified_data, kpi_data):
    """Create the main dashboard"""
    st.markdown('<h1 class="main-header">ğŸ›¡ï¸ Ù„ÙˆØ­Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„</h1>', unsafe_allow_html=True)
    
    # Initialize dashboard components
    dashboard = DashboardComponents()
    
    # Create interactive filters
    filters = dashboard.create_interactive_filters(unified_data)
    
    # Main KPI section
    st.header("ğŸ“Š Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©")
    dashboard.create_kpi_cards(kpi_data)
    
    # Compliance overview
    dashboard.create_compliance_overview(unified_data)
    
    # Risk management section
    dashboard.create_risk_management_section(unified_data)
    
    # Activity heatmap
    dashboard.create_activity_heatmap(unified_data)
    
    # Time series analysis
    dashboard.create_time_series_analysis(unified_data)
    
    # Detailed tables
    dashboard.create_detailed_tables(unified_data, filters)

def create_analytics_page(unified_data, kpi_data):
    """Create advanced analytics page"""
    st.header("ğŸ“ˆ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
    
    tab1, tab2, tab3, tab4 = st.tabs(["ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡", "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø±", "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª", "Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ"])
    
    with tab1:
        st.subheader("ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ")
        
        # Performance metrics by department
        if unified_data:
            performance_data = []
            for data_type, df in unified_data.items():
                if df.empty:
                    continue
                
                # Calculate performance metrics
                dept_col = None
                status_col = None
                
                for col in df.columns:
                    if any(keyword in col.lower() for keyword in ['Ø¥Ø¯Ø§Ø±Ø©', 'Ù‚Ø·Ø§Ø¹', 'department']):
                        dept_col = col
                    elif any(keyword in col.lower() for keyword in ['Ø­Ø§Ù„Ø©', 'status']):
                        status_col = col
                
                if dept_col and status_col:
                    dept_performance = df.groupby(dept_col)[status_col].value_counts().unstack(fill_value=0)
                    
                    for dept in dept_performance.index:
                        closed = dept_performance.loc[dept].get('Ù…ØºÙ„Ù‚', 0)
                        total = dept_performance.loc[dept].sum()
                        compliance_rate = (closed / total * 100) if total > 0 else 0
                        
                        performance_data.append({
                            'Ø§Ù„Ù‚Ø·Ø§Ø¹': dept,
                            'Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª': data_type,
                            'Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª': total,
                            'Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ù…ØºÙ„Ù‚Ø©': closed,
                            'Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„': compliance_rate
                        })
            
            if performance_data:
                performance_df = pd.DataFrame(performance_data)
                
                # Performance comparison chart
                fig = px.sunburst(
                    performance_df,
                    path=['Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª', 'Ø§Ù„Ù‚Ø·Ø§Ø¹'],
                    values='Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø­Ø§Ù„Ø§Øª',
                    color='Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„',
                    color_continuous_scale='RdYlGn',
                    title="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø­Ø³Ø¨ Ø§Ù„Ù‚Ø·Ø§Ø¹ ÙˆÙ†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"
                )
                st.plotly_chart(fig, use_container_width=True)
                
                # Performance table
                st.subheader("Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ")
                st.dataframe(
                    performance_df.sort_values('Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„', ascending=False),
                    use_container_width=True
                )
    
    with tab2:
        st.subheader("ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
        
        if 'risk_assessments' in unified_data and not unified_data['risk_assessments'].empty:
            risk_df = unified_data['risk_assessments']
            
            # Risk correlation matrix
            numeric_cols = risk_df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 1:
                correlation_matrix = risk_df[numeric_cols].corr()
                
                fig = px.imshow(
                    correlation_matrix,
                    title="Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ù„Ù„Ù…Ø®Ø§Ø·Ø±",
                    color_continuous_scale='RdBu',
                    aspect='auto'
                )
                st.plotly_chart(fig, use_container_width=True)
            
            # Risk distribution by activity
            activity_col = None
            for col in risk_df.columns:
                if any(keyword in col.lower() for keyword in ['Ù†Ø´Ø§Ø·', 'activity']):
                    activity_col = col
                    break
            
            if activity_col:
                risk_by_activity = risk_df[activity_col].value_counts()
                
                fig = px.treemap(
                    values=risk_by_activity.values,
                    names=risk_by_activity.index,
                    title="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ø´Ø§Ø·"
                )
                st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ù…ØªØ§Ø­Ø©")
    
    with tab3:
        st.subheader("ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©")
        
        # Combined time series for all data types
        time_series_data = []
        
        for data_type, df in unified_data.items():
            if df.empty:
                continue
            
            date_col = None
            for col in df.columns:
                if df[col].dtype == 'datetime64[ns]':
                    date_col = col
                    break
            
            if date_col:
                monthly_counts = df.groupby(pd.Grouper(key=date_col, freq='M')).size().reset_index()
                monthly_counts['Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª'] = data_type
                monthly_counts.columns = ['Ø§Ù„ØªØ§Ø±ÙŠØ®', 'Ø§Ù„Ø¹Ø¯Ø¯', 'Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª']
                time_series_data.append(monthly_counts)
        
        if time_series_data:
            combined_ts = pd.concat(time_series_data, ignore_index=True)
            
            # Interactive time series chart
            fig = px.line(
                combined_ts,
                x='Ø§Ù„ØªØ§Ø±ÙŠØ®',
                y='Ø§Ù„Ø¹Ø¯Ø¯',
                color='Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª',
                title="Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù„Ø¬Ù…ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
                markers=True
            )
            fig.update_layout(hovermode='x unified')
            st.plotly_chart(fig, use_container_width=True)
            
            # Seasonal analysis
            if len(combined_ts) > 12:
                combined_ts['Ø§Ù„Ø´Ù‡Ø±'] = combined_ts['Ø§Ù„ØªØ§Ø±ÙŠØ®'].dt.month
                seasonal_data = combined_ts.groupby(['Ø§Ù„Ø´Ù‡Ø±', 'Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª'])['Ø§Ù„Ø¹Ø¯Ø¯'].mean().reset_index()
                
                fig = px.bar(
                    seasonal_data,
                    x='Ø§Ù„Ø´Ù‡Ø±',
                    y='Ø§Ù„Ø¹Ø¯Ø¯',
                    color='Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª',
                    title="Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙˆØ³Ù…ÙŠ - Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¹Ø¯Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ø´Ù‡Ø±",
                    barmode='group'
                )
                st.plotly_chart(fig, use_container_width=True)
    
    with tab4:
        st.subheader("Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ")
        
        st.info("Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬ ØªÙ†Ø¨Ø¤ÙŠØ© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„ØªÙˆÙ‚Ø¹ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©")
        
        # Simple trend prediction
        if time_series_data:
            combined_ts = pd.concat(time_series_data, ignore_index=True)
            
            # Group by data type for prediction
            for data_type in combined_ts['Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª'].unique():
                type_data = combined_ts[combined_ts['Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª'] == data_type].copy()
                type_data = type_data.sort_values('Ø§Ù„ØªØ§Ø±ÙŠØ®')
                
                if len(type_data) > 3:
                    # Simple linear trend
                    type_data['trend'] = np.arange(len(type_data))
                    correlation = np.corrcoef(type_data['trend'], type_data['Ø§Ù„Ø¹Ø¯Ø¯'])[0, 1]
                    
                    # Predict next 3 months
                    last_date = type_data['Ø§Ù„ØªØ§Ø±ÙŠØ®'].max()
                    future_dates = pd.date_range(start=last_date + pd.DateOffset(months=1), periods=3, freq='M')
                    
                    # Simple linear extrapolation
                    if abs(correlation) > 0.3:  # Only if there's some correlation
                        slope = np.polyfit(type_data['trend'], type_data['Ø§Ù„Ø¹Ø¯Ø¯'], 1)[0]
                        last_value = type_data['Ø§Ù„Ø¹Ø¯Ø¯'].iloc[-1]
                        
                        predictions = []
                        for i, future_date in enumerate(future_dates, 1):
                            predicted_value = max(0, last_value + slope * i)  # Ensure non-negative
                            predictions.append({
                                'Ø§Ù„ØªØ§Ø±ÙŠØ®': future_date,
                                'Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹': predicted_value,
                                'Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª': data_type
                            })
                        
                        if predictions:
                            pred_df = pd.DataFrame(predictions)
                            
                            # Combine historical and predicted data
                            historical = type_data[['Ø§Ù„ØªØ§Ø±ÙŠØ®', 'Ø§Ù„Ø¹Ø¯Ø¯', 'Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª']].copy()
                            historical['Ø§Ù„Ù†ÙˆØ¹'] = 'ÙØ¹Ù„ÙŠ'
                            historical = historical.rename(columns={'Ø§Ù„Ø¹Ø¯Ø¯': 'Ø§Ù„Ù‚ÙŠÙ…Ø©'})
                            
                            predicted = pred_df[['Ø§Ù„ØªØ§Ø±ÙŠØ®', 'Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹', 'Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª']].copy()
                            predicted['Ø§Ù„Ù†ÙˆØ¹'] = 'Ù…ØªÙˆÙ‚Ø¹'
                            predicted = predicted.rename(columns={'Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹': 'Ø§Ù„Ù‚ÙŠÙ…Ø©'})
                            
                            combined_pred = pd.concat([historical, predicted], ignore_index=True)
                            
                            fig = px.line(
                                combined_pred,
                                x='Ø§Ù„ØªØ§Ø±ÙŠØ®',
                                y='Ø§Ù„Ù‚ÙŠÙ…Ø©',
                                color='Ø§Ù„Ù†ÙˆØ¹',
                                title=f"Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù„Ø£Ø´Ù‡Ø± Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© - {data_type}",
                                line_dash='Ø§Ù„Ù†ÙˆØ¹'
                            )
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # Show prediction table
                            st.subheader(f"Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ù„Ù€ {data_type}")
                            st.dataframe(pred_df, use_container_width=True)

def main():
    """Main application function"""
    
    # Initialize session state
    if 'data_loaded' not in st.session_state:
        st.session_state.data_loaded = False
    
    if 'data_updated' not in st.session_state:
        st.session_state.data_updated = False
    
    # Load data
    if not st.session_state.data_loaded or st.session_state.data_updated:
        with st.spinner("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª..."):
            try:
                processor, unified_data, kpi_data, quality_report = load_and_process_data()
                
                # Merge with uploaded data if available
                if 'uploaded_data' in st.session_state:
                    additional_unified = processor.create_unified_dataset(st.session_state.uploaded_data)
                    
                    # Merge datasets
                    for key, df in additional_unified.items():
                        if key in unified_data:
                            unified_data[key] = pd.concat([unified_data[key], df], ignore_index=True)
                        else:
                            unified_data[key] = df
                    
                    # Regenerate KPIs
                    kpi_data = processor.generate_kpi_data(unified_data)
                
                st.session_state.processor = processor
                st.session_state.unified_data = unified_data
                st.session_state.kpi_data = kpi_data
                st.session_state.quality_report = quality_report
                st.session_state.data_loaded = True
                st.session_state.data_updated = False
                
            except Exception as e:
                st.error(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {str(e)}")
                st.stop()
    
    # Get data from session state
    unified_data = st.session_state.unified_data
    kpi_data = st.session_state.kpi_data
    quality_report = st.session_state.quality_report
    
    # Sidebar navigation
    st.sidebar.title("ğŸ§­ Ø§Ù„ØªÙ†Ù‚Ù„")
    page = st.sidebar.selectbox(
        "Ø§Ø®ØªØ± Ø§Ù„ØµÙØ­Ø©",
        ["Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©", "Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©", "Ø±ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ", "ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©"]
    )
    
    # Display selected page
    if page == "Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©":
        create_main_dashboard(unified_data, kpi_data)
    
    elif page == "Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©":
        create_analytics_page(unified_data, kpi_data)
    
    elif page == "Ø±ÙØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª":
        create_manual_upload_section()
    
    elif page == "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ":
        create_chatbot_interface(unified_data, kpi_data)
    
    elif page == "ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©":
        st.header("ğŸ“‹ ØªÙ‚Ø±ÙŠØ± Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
        
        if quality_report:
            # Overall summary
            total_records = sum([report['total_rows'] for report in quality_report.values()])
            total_columns = sum([report['total_columns'] for report in quality_report.values()])
            avg_missing = np.mean([report['missing_data_percentage'] for report in quality_report.values()])
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª", f"{total_records:,}")
            with col2:
                st.metric("Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©", total_columns)
            with col3:
                st.metric("Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©", f"{avg_missing:.1f}%")
            
            # Detailed report for each dataset
            for data_type, report in quality_report.items():
                with st.expander(f"ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„: {data_type}"):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.subheader("Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©")
                        st.write(f"**Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ:** {report['total_rows']:,}")
                        st.write(f"**Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:** {report['total_columns']}")
                        st.write(f"**Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©:** {report['missing_data_percentage']:.1f}%")
                        st.write(f"**Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ù…ÙƒØ±Ø±Ø©:** {report['duplicate_rows']:,}")
                        st.write(f"**Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©:** {report['memory_usage'] / 1024:.1f} KB")
                    
                    with col2:
                        st.subheader("Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
                        data_types_df = pd.DataFrame([
                            {'Ø§Ù„Ø¹Ù…ÙˆØ¯': col, 'Ø§Ù„Ù†ÙˆØ¹': str(dtype)}
                            for col, dtype in report['data_types'].items()
                        ])
                        st.dataframe(data_types_df, use_container_width=True)
        else:
            st.warning("Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØªÙ‚Ø±ÙŠØ± Ø¬ÙˆØ¯Ø© Ù…ØªØ§Ø­")
    
    # Footer
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666; padding: 1rem;'>
            <p>ğŸ›¡ï¸ Ù„ÙˆØ­Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„ | ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ</p>
            <p>Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«: {}</p>
        </div>
        """.format(datetime.now().strftime("%Y-%m-%d %H:%M")),
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
        """Clean column names and handle multi-level headers"""
        if len(df) > 0:
            first_row = df.iloc[0]
            new_columns = []
            
            for i, (col, val) in enumerate(zip(df.columns, first_row)):
                if pd.notna(val) and str(val).strip() != '':
                    new_columns.append(str(val).strip())
                elif col != f'Unnamed: {i}':
                    new_columns.append(col)
                else:
                    new_columns.append(f'Column_{i}')
            
            df.columns = new_columns
            df = df.drop(df.index[0]).reset_index(drop=True)
        
        return df

    def standardize_status(status_value):
        """Standardize status values"""
        if pd.isna(status_value):
            return None
        
        status_str = str(status_value).strip().lower()
        if 'open' in status_str or 'Ù…ÙØªÙˆØ­' in status_str:
            return 'Open'
        elif 'close' in status_str or 'Ù…ØºÙ„Ù‚' in status_str:
            return 'Closed'
        else:
            return status_value

    def standardize_classification(classification_value):
        """Standardize classification values"""
        if pd.isna(classification_value):
            return None
        
        class_str = str(classification_value).strip().lower()
        if 'high' in class_str or 'Ø¹Ø§Ù„ÙŠ' in class_str:
            return 'High'
        elif 'medium' in class_str or 'Ù…ØªÙˆØ³Ø·' in class_str:
            return 'Medium'
        elif 'low' in class_str or 'Ù…Ù†Ø®ÙØ¶' in class_str:
            return 'Low'
        else:
            return classification_value

    def clean_date_column_safe(date_series):
        """Clean and standardize date columns safely"""
        try:
            return pd.to_datetime(date_series, errors='coerce')
        except:
            return date_series

    def clean_duplicate_columns(df):
        """Handle duplicate column names"""
        cols = pd.Series(df.columns)
        for dup in cols[cols.duplicated()].unique():
            cols[cols[cols == dup].index.values.tolist()] = [dup + '_' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]
        df.columns = cols
        return df

    # Load and clean all datasets
    cleaned_datasets = {}
    
    # Sheet mapping
    sheets_info = {
        'site_audits': 'ØªÙ‚Ø§Ø±ÙŠØ± ØªØ¯Ù‚ÙŠÙ‚ ÙˆÙØ­Øµ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹',
        'risk_assessment': 'ØªÙˆØµÙŠØ§Øª ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®Ø§Ø·Ø±',
        'contractor_audits': 'ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù‚Ø§ÙˆÙ„ÙŠÙ†',
        'incidents': 'ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø­ÙˆØ§Ø¯Ø«',
        'hypotheses': 'ØªÙˆØµÙŠØ§Øª Ø§Ù„ÙØ±Ø¶ÙŠØ§Øª',
        'fire_safety': 'ÙØ­Øµ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙˆØ§Ù„Ø¥Ø·ÙØ§Ø¡',
        'inspection_notes': 'Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„ØªÙØªÙŠØ´',
        'scis_audit': 'ØªØ¯Ù‚ÙŠÙ‚ Ù…ØªØ·Ù„Ø¨Ø§Øª SCIS'
    }
    
    for key, sheet_name in sheets_info.items():
        try:
            df = pd.read_excel('sample-of-data.xlsx', sheet_name=sheet_name)
            df_clean = clean_column_names(df, sheet_name)
            df_clean = clean_duplicate_columns(df_clean)
            
            # Standardize status columns
            status_cols = [col for col in df_clean.columns if 'Ø­Ø§Ù„Ø©' in col]
            for col in status_cols:
                df_clean[col] = df_clean[col].apply(standardize_status)
            
            # Standardize classification columns
            class_cols = [col for col in df_clean.columns if 'Ø§Ù„ØªØµÙ†ÙŠÙ' in col]
            for col in class_cols:
                df_clean[col] = df_clean[col].apply(standardize_classification)
            
            # Clean date columns
            date_columns = [col for col in df_clean.columns if 'ØªØ§Ø±ÙŠØ®' in col]
            for col in date_columns:
                df_clean[col] = clean_date_column_safe(df_clean[col])
            
            cleaned_datasets[key] = df_clean
            
        except Exception as e:
            st.error(f"Error loading {sheet_name}: {str(e)}")
    
    return cleaned_datasets

@st.cache_data
def calculate_kpis(datasets):
    """Calculate key performance indicators"""
    
    # Collect all status data
    all_statuses = []
    for key, df in datasets.items():
        status_cols = [col for col in df.columns if 'Ø­Ø§Ù„Ø©' in col and 'Status' not in col]
        for col in status_cols:
            statuses = df[col].dropna().tolist()
            all_statuses.extend([(status, key) for status in statuses])
    
    status_df = pd.DataFrame(all_statuses, columns=['Status', 'Dataset'])
    
    # Calculate closing compliance
    total_open = status_df[status_df['Status'].isin(['Open', 'Ù…ÙØªÙˆØ­ - Open'])].shape[0]
    total_closed = status_df[status_df['Status'].isin(['Closed', 'Ù…ØºÙ„Ù‚ - Close'])].shape[0]
    total_items = total_open + total_closed
    
    closing_compliance_rate = (total_closed / total_items) * 100 if total_items > 0 else 0
    
    # Risk management metrics
    risk_data = datasets.get('risk_assessment', pd.DataFrame())
    avg_risk = 0
    high_risk_count = 0
    
    if not risk_data.empty and 'Ù†Ø³Ø¨ Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©' in risk_data.columns:
        avg_risk = risk_data['Ù†Ø³Ø¨ Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©'].mean()
        high_risk_count = risk_data[risk_data['Ù†Ø³Ø¨ Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©'] > 0.7].shape[0]
    
    # Total records
    total_records = sum([df.shape[0] for df in datasets.values()])
    
    return {
        'closing_compliance_rate': closing_compliance_rate,
        'total_open': total_open,
        'total_closed': total_closed,
        'total_items': total_items,
        'avg_risk': avg_risk,
        'high_risk_count': high_risk_count,
        'total_records': total_records,
        'status_df': status_df
    }

def create_sector_performance_chart(datasets):
    """Create sector performance chart"""
    sector_data = []
    
    for key, df in datasets.items():
        dept_cols = [col for col in df.columns if 'Ø§Ù„Ø¥Ø¯Ø§Ø±Ø©' in col and 'Ø§Ù„Ù…Ø³Ø¦ÙˆÙ„Ø©' in col]
        for col in dept_cols:
            depts = df[col].dropna().tolist()
            for dept in depts:
                sector_data.append({'Sector': dept, 'Dataset': key, 'Count': 1})
    
    if sector_data:
        sector_df = pd.DataFrame(sector_data)
        sector_summary = sector_df.groupby('Sector')['Count'].sum().reset_index()
        sector_summary = sector_summary.sort_values('Count', ascending=True)
        
        fig = px.bar(
            sector_summary, 
            x='Count', 
            y='Sector',
            orientation='h',
            title='Sector Performance (Total Items)',
            color='Count',
            color_continuous_scale='Blues'
        )
        fig.update_layout(height=400, showlegend=False)
        return fig
    
    return None

def create_activity_distribution_chart(datasets):
    """Create activity type distribution chart"""
    activity_data = []
    
    for key, df in datasets.items():
        activity_cols = [col for col in df.columns if 'ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Ø´Ø§Ø·' in col]
        for col in activity_cols:
            activities = df[col].dropna().tolist()
            for activity in activities:
                # Clean activity names
                clean_activity = activity.split('\n')[0] if '\n' in activity else activity
                activity_data.append({'Activity': clean_activity, 'Dataset': key, 'Count': 1})
    
    if activity_data:
        activity_df = pd.DataFrame(activity_data)
        activity_summary = activity_df.groupby('Activity')['Count'].sum().reset_index()
        activity_summary = activity_summary.sort_values('Count', ascending=False).head(10)
        
        fig = px.bar(
            activity_summary,
            x='Activity',
            y='Count',
            title='Top 10 Activity Types',
            color='Count',
            color_continuous_scale='Viridis'
        )
        fig.update_xaxes(tickangle=45)
        fig.update_layout(height=400, showlegend=False)
        return fig
    
    return None

def create_status_pie_chart(kpis):
    """Create status distribution pie chart"""
    status_counts = kpis['status_df']['Status'].value_counts()
    
    fig = px.pie(
        values=status_counts.values,
        names=status_counts.index,
        title='Status Distribution',
        color_discrete_sequence=px.colors.qualitative.Set3
    )
    fig.update_layout(height=400)
    return fig

def create_risk_distribution_chart(datasets):
    """Create risk level distribution chart"""
    risk_data = datasets.get('risk_assessment', pd.DataFrame())
    
    if not risk_data.empty and 'Ù†Ø³Ø¨ Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©' in risk_data.columns:
        # Create risk level categories
        risk_data_copy = risk_data.copy()
        risk_data_copy['Risk_Category'] = pd.cut(
            risk_data_copy['Ù†Ø³Ø¨ Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©'],
            bins=[0, 0.3, 0.7, 1.0],
            labels=['Low Risk', 'Medium Risk', 'High Risk']
        )
        
        risk_counts = risk_data_copy['Risk_Category'].value_counts()
        
        fig = px.bar(
            x=risk_counts.index,
            y=risk_counts.values,
            title='Risk Level Distribution',
            color=risk_counts.values,
            color_continuous_scale='Reds'
        )
        fig.update_layout(height=400, showlegend=False)
        return fig
    
    return None

def create_unit_performance_chart(datasets):
    """Create unit performance comparison chart"""
    unit_data = []
    
    for key, df in datasets.items():
        unit_cols = [col for col in df.columns if 'Ø§Ù„ÙˆØ­Ø¯Ø©' in col]
        for col in unit_cols:
            units = df[col].dropna().tolist()
            for unit in units:
                unit_data.append({'Unit': unit, 'Dataset': key, 'Count': 1})
    
    if unit_data:
        unit_df = pd.DataFrame(unit_data)
        unit_summary = unit_df.groupby(['Unit', 'Dataset'])['Count'].sum().reset_index()
        
        fig = px.bar(
            unit_summary,
            x='Unit',
            y='Count',
            color='Dataset',
            title='Unit Performance by Dataset',
            barmode='stack'
        )
        fig.update_layout(height=400)
        return fig
    
    return None

def main():
    """Main Streamlit application"""
    
    # Header
    st.markdown('<h1 class="main-header">ğŸ›¡ï¸ Safety & Compliance Dashboard</h1>', unsafe_allow_html=True)
    
    # Load data
    with st.spinner('Loading and processing data...'):
        datasets = load_and_clean_data()
        kpis = calculate_kpis(datasets)
    
    # Sidebar filters
    st.sidebar.header("ğŸ“Š Filters")
    
    # Dataset filter
    available_datasets = list(datasets.keys())
    selected_datasets = st.sidebar.multiselect(
        "Select Datasets",
        available_datasets,
        default=available_datasets
    )
    
    # Filter datasets based on selection
    filtered_datasets = {k: v for k, v in datasets.items() if k in selected_datasets}
    
    # Recalculate KPIs for filtered data
    filtered_kpis = calculate_kpis(filtered_datasets)
    
    # Main dashboard
    st.header("ğŸ“ˆ Key Performance Indicators")
    
    # KPI Cards
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            label="Closing Compliance Rate",
            value=f"{filtered_kpis['closing_compliance_rate']:.1f}%",
            delta=f"{filtered_kpis['total_closed']} closed"
        )
    
    with col2:
        st.metric(
            label="Total Items",
            value=filtered_kpis['total_items'],
            delta=f"{filtered_kpis['total_open']} open"
        )
    
    with col3:
        st.metric(
            label="Average Risk Level",
            value=f"{filtered_kpis['avg_risk']:.2f}",
            delta=f"{filtered_kpis['high_risk_count']} high risk"
        )
    
    with col4:
        st.metric(
            label="Total Records",
            value=filtered_kpis['total_records'],
            delta=f"{len(filtered_datasets)} datasets"
        )
    
    # Charts section
    st.header("ğŸ“Š Analytics Dashboard")
    
    # Row 1: Sector Performance and Status Distribution
    col1, col2 = st.columns(2)
    
    with col1:
        sector_chart = create_sector_performance_chart(filtered_datasets)
        if sector_chart:
            st.plotly_chart(sector_chart, use_container_width=True)
        else:
            st.info("No sector data available for selected datasets")
    
    with col2:
        status_chart = create_status_pie_chart(filtered_kpis)
        st.plotly_chart(status_chart, use_container_width=True)
    
    # Row 2: Activity Distribution and Risk Distribution
    col1, col2 = st.columns(2)
    
    with col1:
        activity_chart = create_activity_distribution_chart(filtered_datasets)
        if activity_chart:
            st.plotly_chart(activity_chart, use_container_width=True)
        else:
            st.info("No activity data available for selected datasets")
    
    with col2:
        risk_chart = create_risk_distribution_chart(filtered_datasets)
        if risk_chart:
            st.plotly_chart(risk_chart, use_container_width=True)
        else:
            st.info("No risk data available for selected datasets")
    
    # Row 3: Unit Performance
    st.subheader("Unit Performance Analysis")
    unit_chart = create_unit_performance_chart(filtered_datasets)
    if unit_chart:
        st.plotly_chart(unit_chart, use_container_width=True)
    else:
        st.info("No unit data available for selected datasets")
    
    # Reports Section
    st.header("ğŸ“‹ Comprehensive Reports")
    
    # Create tabs for different report sections
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“Š Data Overview", "ğŸ” Detailed Analysis", "ğŸ“ˆ Trends", "ğŸ“‹ Raw Data"])
    
    with tab1:
        st.subheader("Data Overview")
        
        # Dataset summary table
        summary_data = []
        for key, df in filtered_datasets.items():
            summary_data.append({
                'Dataset': key.replace('_', ' ').title(),
                'Records': df.shape[0],
                'Columns': df.shape[1],
                'Date Range': f"{df.select_dtypes(include=['datetime64']).min().min():%Y-%m-%d} to {df.select_dtypes(include=['datetime64']).max().max():%Y-%m-%d}" if not df.select_dtypes(include=['datetime64']).empty else "N/A"
            })
        
        summary_df = pd.DataFrame(summary_data)
        st.dataframe(summary_df, use_container_width=True)
        
        # Key insights
        st.subheader("Key Insights")
        st.write(f"â€¢ **Total compliance rate**: {filtered_kpis['closing_compliance_rate']:.1f}% of items are closed")
        st.write(f"â€¢ **Risk management**: Average risk level is {filtered_kpis['avg_risk']:.2f} with {filtered_kpis['high_risk_count']} high-risk items")
        st.write(f"â€¢ **Data coverage**: {filtered_kpis['total_records']} total records across {len(filtered_datasets)} datasets")
    
    with tab2:
        st.subheader("Detailed Analysis")
        
        # Compliance by sector
        st.write("**Compliance Analysis by Sector**")
        sector_compliance = []
        
        for key, df in filtered_datasets.items():
            status_cols = [col for col in df.columns if 'Ø­Ø§Ù„Ø©' in col and 'Status' not in col]
            dept_cols = [col for col in df.columns if 'Ø§Ù„Ø¥Ø¯Ø§Ø±Ø©' in col and 'Ø§Ù„Ù…Ø³Ø¦ÙˆÙ„Ø©' in col]
            
            if status_cols and dept_cols:
                for _, row in df.iterrows():
                    status = row[status_cols[0]] if status_cols else None
                    dept = row[dept_cols[0]] if dept_cols else None
                    
                    if pd.notna(status) and pd.notna(dept):
                        sector_compliance.append({
                            'Sector': dept,
                            'Status': status,
                            'Dataset': key
                        })
        
        if sector_compliance:
            compliance_df = pd.DataFrame(sector_compliance)
            compliance_summary = compliance_df.groupby(['Sector', 'Status']).size().unstack(fill_value=0)
            
            if 'Closed' in compliance_summary.columns and 'Open' in compliance_summary.columns:
                compliance_summary['Compliance_Rate'] = (compliance_summary['Closed'] / (compliance_summary['Closed'] + compliance_summary['Open']) * 100).round(1)
                st.dataframe(compliance_summary, use_container_width=True)
            else:
                st.dataframe(compliance_summary, use_container_width=True)
        
        # Top issues analysis
        st.write("**Top Issues by Activity Type**")
        if 'activity_data' in locals():
            activity_issues = pd.DataFrame(activity_data)
            top_issues = activity_issues.groupby('Activity')['Count'].sum().sort_values(ascending=False).head(10)
            st.bar_chart(top_issues)
    
    with tab3:
        st.subheader("Trend Analysis")
        
        # Monthly trend analysis
        monthly_data = []
        for key, df in filtered_datasets.items():
            date_cols = [col for col in df.columns if 'ØªØ§Ø±ÙŠØ®' in col]
            if date_cols:
                for _, row in df.iterrows():
                    date_val = row[date_cols[0]]
                    if pd.notna(date_val):
                        monthly_data.append({
                            'Date': date_val,
                            'Dataset': key,
                            'Count': 1
                        })
        
        if monthly_data:
            trend_df = pd.DataFrame(monthly_data)
            trend_df['Month'] = pd.to_datetime(trend_df['Date']).dt.to_period('M')
            monthly_summary = trend_df.groupby(['Month', 'Dataset'])['Count'].sum().reset_index()
            monthly_summary['Month'] = monthly_summary['Month'].astype(str)
            
            fig = px.line(
                monthly_summary,
                x='Month',
                y='Count',
                color='Dataset',
                title='Monthly Trend Analysis',
                markers=True
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No date data available for trend analysis")
    
    with tab4:
        st.subheader("Raw Data Explorer")
        
        # Dataset selector for raw data
        selected_dataset = st.selectbox(
            "Select dataset to view:",
            options=list(filtered_datasets.keys()),
            format_func=lambda x: x.replace('_', ' ').title()
        )
        
        if selected_dataset:
            df = filtered_datasets[selected_dataset]
            
            # Display basic info
            st.write(f"**Dataset**: {selected_dataset.replace('_', ' ').title()}")
            st.write(f"**Shape**: {df.shape[0]} rows Ã— {df.shape[1]} columns")
            
            # Column selector
            selected_columns = st.multiselect(
                "Select columns to display:",
                options=df.columns.tolist(),
                default=df.columns.tolist()[:10] if len(df.columns) > 10 else df.columns.tolist()
            )
            
            if selected_columns:
                # Display filtered data
                st.dataframe(df[selected_columns], use_container_width=True)
                
                # Download button
                csv = df[selected_columns].to_csv(index=False)
                st.download_button(
                    label="Download CSV",
                    data=csv,
                    file_name=f"{selected_dataset}_data.csv",
                    mime="text/csv"
                )
    
    # Footer
    st.markdown("---")
    st.markdown("**Safety & Compliance Dashboard** | Built with Streamlit | Data updated: " + datetime.now().strftime("%Y-%m-%d %H:%M"))

if __name__ == "__main__":
    main()